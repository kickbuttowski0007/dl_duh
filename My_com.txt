hdfs architecture
Hadoop Distributed File System (HDFS) is a distributed file system designed to store and manage large volumes of data across a cluster of commodity hardware. The HDFS architecture consists of the following components:
1. NameNode: The NameNode is the master node that manages the file system namespace and regulates access to files by clients. It maintains metadata information about the files, such as the file name, location, and access permissions. The NameNode also manages the replication factor for the files, which specifies the number of copies of a file that should be stored in the cluster.
2. DataNode: The DataNode is the worker node that stores the actual data blocks of the files. It manages the storage of data on the local disk and communicates with the NameNode to provide information about the data blocks it stores.
3. Secondary NameNode: The Secondary NameNode is not a backup or standby NameNode. It periodically merges the edit log and fsimage files to generate a new image for the NameNode to ensure that the NameNode can recover from failures.
4. Client: The Client is any application that accesses the HDFS files system, such as Hadoop MapReduce, Hive, and Pig.
The HDFS architecture also includes a few other components, such as:
1. Block: HDFS divides the data into smaller blocks and stores each block on different DataNodes in the cluster. By default, the block size is 128 MB, but it can be configured based on the needs of the application.
2. Rack: A rack is a collection of DataNodes that are located in the same physical location, typically a single rack or switch.
3. Replica: HDFS stores multiple copies of each block in different DataNodes in the cluster. The number of replicas can be configured by the administrator.
Overall, the HDFS architecture is designed to provide high fault tolerance, high availability, and scalability for large-scale distributed data processing.
 
hdfs read
Reading data from HDFS involves the following steps:
1. The client application requests data from the HDFS by sending a read request to the NameNode.
2. The NameNode identifies the DataNodes that contain the requested data blocks and returns the location information to the client.
3. The client connects to one of the identified DataNodes and requests the data block.
4. The DataNode retrieves the data block from its local disk and sends it back to the client.
5. If the requested data block is not available on the first DataNode, the client may request the data from other DataNodes that store replicas of the data block.
6. Once the client has received all the data blocks, it can merge them together to form the complete file.
To read data from HDFS, you can use various APIs provided by Hadoop, such as the HDFS API, Hadoop Streaming, and Hadoop MapReduce. The HDFS API provides low-level access to the file system, allowing you to read and write files directly. The Hadoop Streaming API allows you to use any programming language to read and write data to HDFS. The Hadoop MapReduce framework provides a high-level interface for distributed data processing, which can be used to read and process data stored in HDFS.
Overall, reading data from HDFS is a straightforward process, and Hadoop provides several APIs to make it easy for developers to work with large volumes of data stored in HDFS.
 
hdfs write
Writing data to HDFS involves the following steps:
1. The client application requests to create a new file in HDFS by sending a write request to the NameNode.
2. The NameNode checks if the file already exists and if it does not, it creates a new entry in the file system namespace.
3. The NameNode identifies the DataNodes where the replicas of the data blocks will be stored and returns the location information to the client.
4. The client connects to one of the identified DataNodes and sends the data block to be written.
5. The DataNode writes the data block to its local disk and sends an acknowledgement back to the client.
6. Once the first data block is written, the client may request to write subsequent data blocks to other DataNodes that store replicas of the data.
7. Once all the data blocks are written, the client signals to the NameNode to close the file.
To write data to HDFS, you can use various APIs provided by Hadoop, such as the HDFS API, Hadoop Streaming, and Hadoop MapReduce. The HDFS API provides low-level access to the file system, allowing you to read and write files directly. The Hadoop Streaming API allows you to use any programming language to read and write data to HDFS. The Hadoop MapReduce framework provides a high-level interface for distributed data processing, which can be used to write data to HDFS.
Overall, writing data to HDFS is a straightforward process, and Hadoop provides several APIs to make it easy for developers to work with large volumes of data stored in HDFS. HDFS provides high fault tolerance and data replication, ensuring that the data is safe and highly available even in the event of hardware or network failures.
 
mapreduce in action
MapReduce is a programming model and framework for processing large datasets in parallel across a distributed cluster of computers. It is designed to be fault-tolerant, scalable, and highly efficient for batch processing of large datasets.
Here's an example of MapReduce in action:
Suppose you have a large dataset consisting of web server logs and you want to count the number of times each IP address appears in the logs. You can use MapReduce to process this data in parallel across a distributed cluster of computers. Here's how you might implement this in MapReduce:
1. Map Phase: In the map phase, each data node processes a subset of the input data and generates intermediate key-value pairs. In this example, the map function would parse each log file and emit a key-value pair for each IP address it finds, with the IP address as the key and a count of 1 as the value.
2. Shuffle Phase: In the shuffle phase, the intermediate key-value pairs generated by the map function are sorted and partitioned based on their keys, and sent to the appropriate reducer nodes.
3. Reduce Phase: In the reduce phase, each reducer node receives a subset of the intermediate key-value pairs and performs a reduction operation on them. In this example, the reduce function would receive all the key-value pairs with a particular IP address as the key and would sum the counts to get the total number of times that IP address appears in the logs.
4. Output: The final output of the MapReduce job would be a set of key-value pairs, with the key being the IP address and the value being the total count.
To run this MapReduce job in Hadoop, you would need to write Map and Reduce functions in Java, and configure the job using the Hadoop MapReduce API. You would then submit the job to the Hadoop cluster, which would distribute the input data and run the map and reduce tasks in parallel across the nodes in the cluster.
Overall, MapReduce provides a powerful and flexible framework for processing large datasets in parallel, and is widely used for data analytics, machine learning, and other data-intensive applications.
 
MRv1
MRv1, also known as Hadoop MapReduce version 1, is the original implementation of the MapReduce framework in Hadoop. It was the first production-ready version of MapReduce released by Apache Hadoop in 2009 and was the primary processing framework for Hadoop until the release of MRv2, also known as YARN, in 2012.
MRv1 has a simple architecture with a single JobTracker and multiple TaskTrackers. The JobTracker is responsible for managing and scheduling jobs, while the TaskTrackers are responsible for executing tasks assigned to them by the JobTracker. The data processing is performed in a series of map and reduce tasks, with the map tasks processing input data and generating intermediate key-value pairs, and the reduce tasks processing the intermediate data and producing the final output.
One of the main drawbacks of MRv1 is that it has limited scalability and is not suitable for running large-scale, complex workloads. This is because the JobTracker is a single point of failure and can become a bottleneck for scheduling and coordinating tasks. Additionally, MRv1 does not support resource allocation and management, making it difficult to allocate resources optimally.
Despite its limitations, MRv1 is still widely used in some legacy applications and is supported by many Hadoop distributions. However, most new applications are now developed using MRv2/YARN or higher-level processing frameworks such as Apache Spark or Apache Flink, which offer greater scalability, flexibility, and performance.
A Client invokes a Map
Reduce, from a Calling Node (maybe a Data Node or an Extra Node in the cluster)

An instance of Job Tracker is created in the memory of the Calling Node.

The Job Tracker queries the Name Node and finds the Data Nodes (location of the data to be used).

Job Tracker then launches Task Trackers in the memory of all the Data Nodes as above to run the jobs

Job Tracker gives the code to Task Tracker to run as a Task

Task Tracker is responsible for creating the tasks & running the tasks

In effect the Mapper of the Job is found here

Once the Task is completed, the result from the Tasks is sent back to the Job Tracker

Job Tracker also keeps a track of progress by each Task Tracker

The Job Tracker also receives the results from each Task Tracker and aggregates the results

In effect the Reducer of the Job is found here

 
MRv2
MRv2, also known as Hadoop MapReduce version 2, is the current implementation of the MapReduce framework in Hadoop. It was introduced in 2012 as a major update to MRv1 and is now the primary processing framework for Hadoop.
The main difference between MRv1 and MRv2 is that MRv2 is built on top of YARN (Yet Another Resource Negotiator), which is a distributed operating system for Hadoop. YARN provides a more scalable and flexible architecture for managing resources and scheduling jobs, compared to the single JobTracker architecture of MRv1.
In MRv2/YARN, the processing of MapReduce jobs is divided into two distinct phases:
1. Resource Negotiation: The client application negotiates with the ResourceManager to acquire resources for running the job. The ResourceManager allocates containers on available NodeManagers based on the resource requirements of the job.
2. Job Execution: The containers are used to execute the MapReduce tasks. The MapReduce ApplicationMaster runs as a container and manages the execution of the job. The MapReduce tasks themselves run as containers on the NodeManagers.
This architecture allows for better scalability and resource utilization, as it separates the responsibility of resource management from the job execution. It also allows for the coexistence of multiple processing frameworks, such as Spark, Flink, and Tez, on the same cluster, as they can all share the same YARN resource management infrastructure.
In addition to YARN integration, MRv2 also includes several other improvements over MRv1, such as support for input/output formats, counters, distributed cache, and pluggable serialization. MRv2 is also backward compatible with MRv1, so applications developed for MRv1 can be easily migrated to MRv2.
Overall, MRv2 provides a more scalable and flexible architecture for processing large-scale data in Hadoop, and is widely used in many data-intensive applications.
A Client invokes a Map
Reduce, from a Calling Node (maybe a Data Node or an Extra Node in the cluster)

An instance of Resource Manager is created in the memory of the Calling Node.

The Resource Manager then
launches containers with appropriate resources (memory) with App Node
Manager in memory of the Calling Node

Along with this Application Master is invoked. Application Master is “pause” mode till all containers
with
Task Node Manager (as below) are created.

The Resource Manager queries the Name Node and finds the Data Nodes (location of the data used).

The Resource Manager then
launches containers with appropriate resources (memory) with Task Node
Manager in all the Data Nodes as above to run the jobs

Application Master gives the code to Task Node Manager to run as a Task

Task Node Manager is responsible for creating & running tasks. In effect the Mapper of the Job is here

Once the Task is completed, the result from the Tasks is sent back to the Application Master

Application Master also keeps a track of progress by each Task Node Manager

The Application Master
lso receives the results from each Task Node Manager and aggregates the results

In effect the Reducer of the Job is found here

Thus from previous version, Job Tracker has been replaced by Resource Manager & Application Master

From previous version,
TaskTracker has been replaced by Task Node Managers


difference between mrv1 and mrv2
MRv1 and MRv2 (YARN) are two versions of the MapReduce framework in Hadoop with significant differences in their architecture and capabilities. Here are some of the key differences between MRv1 and MRv2:
1. Architecture: MRv1 has a simple architecture with a single JobTracker and multiple TaskTrackers. MRv2 is built on top of YARN and has a more complex architecture with a ResourceManager, NodeManagers, and ApplicationMasters.
2. Scalability: MRv1 has limited scalability due to the single point of failure of the JobTracker and lack of resource management. MRv2/YARN is designed to be highly scalable and can manage a much larger number of nodes and jobs.
3. Resource Management: MRv1 has limited resource management and scheduling capabilities. MRv2/YARN provides more advanced resource management and scheduling features, such as support for multiple resource types, pluggable scheduling policies, and dynamic resource allocation.
4. Compatibility: MRv2 is backward compatible with MRv1, which means that applications developed for MRv1 can be easily migrated to MRv2. However, there are some differences in the API and configuration between the two versions.
5. Performance: MRv2/YARN provides better performance for large-scale data processing compared to MRv1, due to its improved resource management and scheduling capabilities.
Overall, MRv2/YARN provides a more scalable, flexible, and efficient architecture for processing large-scale data compared to MRv1. MRv1 is still used in some legacy applications, but most new applications are now developed using MRv2/YARN or higher-level processing frameworks such as Apache Spark or Apache Flink.
 
map reduce failure recovery
MapReduce failure recovery is an important aspect of the framework as it ensures that jobs can be completed even if there are failures in the system. Here are some of the ways in which MapReduce handles failure recovery:
1. Task-level retries: MapReduce can automatically retry failed map and reduce tasks. When a task fails, the MapReduce framework can automatically reschedule the task on another node in the cluster. The number of retries is configurable, and by default, it is set to 4.
2. Speculative execution: MapReduce can run multiple copies of the same task on different nodes in the cluster. If a task takes longer than expected to complete, the framework can start another copy of the same task on a different node. The first copy to complete is used, and the other copies are terminated. This approach helps to ensure that tasks are completed within a reasonable time, even if some nodes or tasks fail.
3. Task and Job-level monitoring: MapReduce provides a monitoring mechanism that allows the framework to detect task and job failures. If a task or job fails, the monitoring mechanism can trigger a set of actions, such as rescheduling the task or restarting the job.
4. Backup tasks: MapReduce can also use backup tasks, which are tasks that are run in parallel with the original tasks. If a task fails, the backup task can take over and complete the job.
5. Job-level checkpoints: MapReduce can use checkpoints to store intermediate data generated by tasks during job execution. If a task fails, the framework can use the checkpoint to restart the task from where it left off, rather than starting the task from the beginning.
Overall, MapReduce provides several mechanisms to handle failure recovery and ensure that jobs can be completed even in the presence of node failures or other issues. These mechanisms can help to improve the robustness and reliability of large-scale data processing systems.
MRv1

Task Failure
new task is started by the Task Tracker

Task Tracker Failure
new Task Tracker is started by the Job Tracker

Job Tracker Failure
no recovery; single point of failure
MRv2

Task Failure
new task is started by Task Node Manager

Task Node Manager Failure
new container with Task Node Manager is created by Resource Manager
this Task Node Manager is given the code and started by Application Master

Application Master Failure
new Application Master is started by App Node Manager

App Node Manager Failure
new container with App Node Manager is created by Resource Manager
this App Node Manager invokes the Application Master

Resource Manager Failure
new resource manager with saved state is started


apache hive architecture
Apache Hive is a data warehouse system that provides a SQL-like interface to Hadoop Distributed File System (HDFS) and other data sources such as Apache HBase, Amazon S3, and Microsoft Azure Data Lake Storage. Here is a high-level overview of the Apache Hive architecture:
1. Metastore: The Metastore is a central repository that stores metadata about the data stored in Hive. It contains information about the database schemas, tables, partitions, and other relevant information. The Metastore can use a variety of databases, such as MySQL or PostgreSQL, to store the metadata.
2. Hive Query Language (HQL): HQL is a SQL-like language that is used to query the data stored in Hive. HQL queries are translated into MapReduce or Tez jobs, which are executed on the Hadoop cluster.
3. Hive Driver: The Hive Driver is responsible for parsing and compiling HQL queries into executable tasks, which are then submitted to the Hadoop cluster. The Driver also interacts with the Metastore to retrieve metadata about the tables and partitions.
4. Execution Engine: The Execution Engine is responsible for executing the tasks generated by the Hive Driver. Hive supports multiple execution engines, including MapReduce and Tez.
5. Hadoop Distributed File System (HDFS): Hive stores data in HDFS, which is a distributed file system that provides reliable and scalable storage for large-scale data processing.
6. Hadoop Cluster: Hive runs on top of the Hadoop cluster and leverages the Hadoop ecosystem to provide a scalable and fault-tolerant data processing platform.
Overall, the Apache Hive architecture provides a flexible and scalable data warehouse system that can handle large-scale data processing workloads. The Metastore and HQL provide a powerful interface for managing and querying data stored in Hive, while the Hadoop cluster provides a reliable and scalable execution platform for processing the data.
 
apache spark
Apache Spark is an open-source distributed computing system that provides an interface for programming clusters with implicit data parallelism and fault tolerance. Spark is designed to perform in-memory computations and can process large volumes of data in a distributed fashion. Here are some of the key features and components of Apache Spark:
1. Resilient Distributed Datasets (RDDs): RDDs are the fundamental data structure in Spark. RDDs are immutable distributed collections of data that can be processed in parallel across a cluster. RDDs can be created from Hadoop Distributed File System (HDFS) or other data sources, and can be cached in memory to allow for faster processing.
2. Spark Core: Spark Core is the underlying computational engine that provides distributed task scheduling, memory management, and fault recovery. Spark Core provides the runtime environment for Spark applications and manages the distributed processing of RDDs.
3. Spark SQL: Spark SQL is a Spark module that provides a SQL interface for querying structured and semi-structured data. Spark SQL allows users to query data in Spark using SQL commands, and also provides a DataFrame API for working with structured data.
4. Spark Streaming: Spark Streaming is a Spark module that allows for the processing of real-time streaming data. Spark Streaming can process data from a variety of sources, including Kafka, Flume, and HDFS.
5. MLlib: MLlib is a Spark module that provides a library of machine learning algorithms for processing large-scale data. MLlib includes algorithms for classification, regression, clustering, and collaborative filtering.
6. GraphX: GraphX is a Spark module that provides a library for processing graph data. GraphX allows users to create, transform, and query graphs using a set of high-level APIs.
Overall, Apache Spark is a powerful distributed computing system that can be used for a wide range of data processing and analysis tasks. Spark's ability to process data in-memory and provide fault tolerance and data parallelism makes it an efficient and scalable solution for big data processing.
 
difference and similarities between hadoop and spark
Hadoop and Spark are both open-source distributed computing systems designed for processing large volumes of data in a distributed fashion. While they share some similarities, they also have some key differences:
1. Data Processing Model: Hadoop uses the MapReduce data processing model for batch processing, while Spark provides a unified data processing model that supports batch, streaming, interactive, and graph processing.
2. In-Memory Processing: Spark is designed to process data in-memory, whereas Hadoop is designed to process data on disk. This means that Spark can provide faster processing for certain workloads, but may require more memory resources.
3. Ease of Use: Spark is generally considered to be easier to use than Hadoop, as it provides a simpler programming model and a more unified API.
4. Data Storage: Hadoop includes the Hadoop Distributed File System (HDFS) for distributed storage of data, while Spark does not have its own distributed file system and relies on other storage systems such as HDFS, Amazon S3, or Azure Data Lake Storage.
5. Community Support: Both Hadoop and Spark have large and active communities of users and developers, with a wide range of third-party tools and libraries available.
Overall, while Hadoop and Spark share some similarities in their ability to process large volumes of data in a distributed fashion, they have some key differences in their data processing model, in-memory processing, ease of use, and data storage. The choice of which system to use will depend on the specific requirements of the use case and the expertise of the development team.
 
columnar database
A columnar database is a type of database management system (DBMS) that stores data by column rather than by row, as is typically done in a traditional row-based DBMS. In a columnar database, the data for each column is stored contiguously on disk or in memory, allowing for more efficient query processing and faster data retrieval.
Here are some of the key features and benefits of columnar databases:
1. Compression: Columnar databases can be highly compressed, as the data within each column is usually highly repetitive. This can result in significant savings in storage space and can also improve query performance, as fewer disk reads are required to retrieve data.
2. Query Performance: Columnar databases can provide faster query performance than row-based databases, as the data for each column is stored contiguously and can be accessed more efficiently. This can be especially beneficial for complex analytical queries that involve large volumes of data.
3. Schema Flexibility: Columnar databases are schema-flexible, as new columns can be added to the database without changing the existing schema. This can be useful for applications that require frequent schema changes or have large numbers of columns.
4. Parallel Processing: Columnar databases can be designed for parallel processing, allowing for multiple queries to be processed simultaneously. This can result in faster query performance and improved scalability.
5. Analytics and Business Intelligence: Columnar databases are well-suited for analytics and business intelligence applications, as they can handle large volumes of data and provide fast query performance for complex analytical queries.
Overall, columnar databases are a powerful tool for managing large volumes of data and can provide significant benefits in terms of storage efficiency, query performance, schema flexibility, and parallel processing.
A columnar database, also known as a column
oriented database

A DBMS that stores data in columns rather than in rows as RDBMS

The table schema defines only column families, which are the key value pairs.

A table can have multiple column families and each column family can have any number of
columns.

Subsequent column values are stored contiguously on the disk.

Each cell value of the table has a timestamp.

The tables in Columnar database are sorted by row.

The rows of tables in Columnar database are identified by a row
key.
The difference centered around performance, storage and schema modifying techniques.

Efficient write & read data to and from hard disk storage; speeds up the time it takes to return a
query.

Main benefits columnar operations like MIN, MAX, SUM, COUNT and AVG performed very
rapidly.

Columnar Database allows random read & write in Hadoop.

Columnar database fulfills the ACID principles of a database.


Apache Pig is a high-level platform for creating MapReduce programs used with Hadoop. It provides a high-level language, called Pig Latin, for expressing data analysis programs which is then compiled into MapReduce programs. Here is a brief overview of Pig's architecture:
1. Pig Latin: Pig Latin is a high-level scripting language used for expressing data analysis programs. It is a data flow language that allows users to express complex data transformations with minimal programming.
2. Pig Latin Compiler: The Pig Latin Compiler is responsible for compiling the Pig Latin scripts into a sequence of MapReduce programs. The compiler takes the Pig Latin script as input, optimizes it, and generates the corresponding MapReduce program that can be executed on a Hadoop cluster.
3. Execution Engine: The Execution Engine is responsible for executing the generated MapReduce program on a Hadoop cluster. The engine takes care of job scheduling, data partitioning, and task coordination to ensure that the program runs correctly and efficiently.
4. Pig Latin Operators: Pig Latin Operators are used to perform data transformation operations on large datasets. Pig provides a wide range of operators, including filters, joins, aggregations, and sorting, which can be combined to create complex data transformation pipelines.
5. Data Storage: Pig can read and write data from a wide range of data storage systems, including HDFS, HBase, Amazon S3, and local file systems.
6. User-Defined Functions (UDFs): Pig provides the ability to define custom user-defined functions (UDFs) in Java or Python, which can be used in Pig Latin scripts to perform complex data transformations that are not supported by built-in operators.
Overall, Pig's architecture provides a high-level, flexible, and scalable platform for creating MapReduce programs using Pig Latin, allowing users to perform complex data transformations on large datasets with minimal programming.


